@article{maaroufConcatenatedCodesMultiple2023a,
  title = {Concatenated Codes for Multiple Reads of a {{DNA}} Sequence},
  author = {Maarouf, Issam and Lenz, Andreas and Welter, Lorenz and {Wachter-Zeh}, Antonia and Rosnes, Eirik and {i Amat}, Alexandre Graell},
  year = {2023},
  journal = {IEEE Transactions on Information Theory},
  volume = {69},
  number = {2},
  pages = {910--927},
  doi = {10.1109/TIT.2022.3206527},
  keywords = {Achievable information rates,citation,Codes,concatenated codes,Convolutional codes,Decoding,DNA,DNA storage,Encoding,insertion/deletion/substitution (IDS) channel,low-density parity-check (LDPC) code,Parity check codes,polar code,Symbols,synchronization codes}
}

@article{maDeepLearningbasedDetection2024b,
  title = {Deep Learning-Based Detection for Marker Codes over Insertion and Deletion Channels},
  author = {Ma, Guochen and Jiao, Xiaopeng and Mu, Jianjun and Han, Hui and Yang, Yaming},
  year = {2024},
  journal = {IEEE Transactions on Communications},
  volume = {72},
  number = {10},
  pages = {5945--5959},
  doi = {10.1109/TCOMM.2024.3394039},
  keywords = {Bidirectional gated recurrent unit (bi-GRU),Channel models,citation,Codes,Deep learning,deep unfolding,insertions and deletions,marker codes,model-driven deep learning,Simulation,Symbols,Synchronization,Uncertainty}
}



@inproceedings{kargiDeepLearningBased2024b,
  title = {A Deep Learning Based Decoder for Concatenated Coding over Deletion Channels},
  booktitle = {{{ICC}} 2024 - {{IEEE}} International Conference on Communications},
  author = {Karg{\i}, E. Uras and Duman, Tolga M.},
  year = {2024},
  pages = {2797--2802},
  doi = {10.1109/ICC51166.2024.10622561},
  keywords = {Channel estimation,channels with synchronization errors,citation,Codes,Convolutional codes,Decoding,Deep learning,deletion channels,Encoding,GRU,marker codes,RNN,Symbols,Transformers}
}



@inproceedings{devlinBERTPretrainingDeep2019a,
	location = {Minneapolis, Minnesota},
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {https://aclanthology.org/N19-1423/},
	doi = {10.18653/v1/N19-1423},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5 (7.7 point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	eventtitle = {{NAACL}-{HLT} 2019},
	pages = {4171--4186},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	urldate = {2025-01-07},
	date = {2019-06},
	file = {Full Text PDF:/home/ocell/Zotero/storage/D5QK6BZ4/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}


@inproceedings{vaswaniAttentionAllYou2017a,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  keywords = {citation}
}



@inproceedings{bennatanDeepLearningDecoding2018b,
  title = {Deep Learning for Decoding of Linear Codes - a Syndrome-Based Approach},
  booktitle = {2018 {{IEEE}} International Symposium on Information Theory ({{ISIT}})},
  author = {Bennatan, Amir and Choukroun, Yoni and Kisilev, Pavel},
  year = {2018},
  pages = {1595--1599},
  doi = {10.1109/ISIT.2018.8437530},
  keywords = {Biological neural networks,citation,Decoding,Estimation,Recurrent neural networks,Reliability,Training}
}


@inproceedings{choukrounErrorCorrectionCode2022b,
  title = {Error Correction Code Transformer},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Choukroun, Yoni and Wolf, Lior},
  editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  year = {2022},
  volume = {35},
  pages = {38695--38705},
  publisher = {Curran Associates, Inc.},
  keywords = {citation}
}



@inproceedings{chungEmpiricalEvaluationGated2014,
  title = {{Empirical evaluation of gated recurrent neural networks on sequence modeling}},
  booktitle = {{NIPS 2014 workshop on deep learning, december 2014}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  langid = {English (US)},
  keywords = {citation}
}


@inproceedings{park2025crossmpt,
  title = {{{CrossMPT}}: {{Cross-attention}} Message-Passing Transformer for Error Correcting Codes},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  author = {Park, Seong-Joon and Kwak, Hee-Youl and Kim, Sang-Hyo and Kim, Yongjune and No, Jong-Seon},
  year = {2025},
  keywords = {citation}
}


@misc{
girsch2025trace,
title={Trace Reconstruction for {DNA} Data Storage using Language Models},
author={Michael Girsch and Reinhard Heckel},
year={2025},
url={https://openreview.net/forum?id=rkfiJQMFcw}
}

@article{daveyReliableCommunicationChannels2001,
  title = {Reliable Communication over Channels with Insertions, Deletions, and Substitutions},
  author = {Davey, M.C. and Mackay, D.J.C.},
  year = {2001},
  month = feb,
  journal = {IEEE Transactions on Information Theory},
  volume = {47},
  number = {2},
  pages = {687--698},
  issn = {1557-9654},
  doi = {10.1109/18.910582},
  urldate = {2024-07-02},
  abstract = {A new block code is introduced which is capable of correcting multiple insertion, deletion, and substitution errors. The code consists of nonlinear inner codes, which we call "watermark"" codes, concatenated with low-density parity-check codes over nonbinary fields. The inner code allows probabilistic resynchronization and provides soft outputs for the outer decoder, which then completes decoding. We present codes of rate 0.7 and transmitted length 5000 bits that can correct 30 insertion/deletion errors per block. We also present codes of rate 3/14 and length 4600 bits that can correct 450 insertion/deletion errors per block.},
  keywords = {Block codes,citation,fundamental,have read,presentation},
  file = {/home/ocell/Zotero/storage/MS6YEI9D/Davey and Mackay - 2001 - Reliable communication over channels with insertio.pdf;/home/ocell/Zotero/storage/4WK6IEBC/910582.html}
}


@article{bahlOptimalDecodingLinear1974,
  title = {Optimal Decoding of Linear Codes for Minimizing Symbol Error Rate ({{Corresp}}.)},
  author = {Bahl, L. and Cocke, J. and Jelinek, F. and Raviv, J.},
  year = {1974},
  journal = {IEEE Transactions on Information Theory},
  volume = {20},
  number = {2},
  pages = {284--287},
  doi = {10.1109/TIT.1974.1055186}
}



@misc{parkHowMaskError2023,
	title = {How to Mask in Error Correction Code Transformer: Systematic and Double Masking},
	url = {http://arxiv.org/abs/2308.08128},
	doi = {10.48550/arXiv.2308.08128},
	shorttitle = {How to Mask in Error Correction Code Transformer},
	abstract = {In communication and storage systems, error correction codes ({ECCs}) are pivotal in ensuring data reliability. As deep learning's applicability has broadened across diverse domains, there is a growing research focus on neural network-based decoders that outperform traditional decoding algorithms. Among these neural decoders, Error Correction Code Transformer ({ECCT}) has achieved the state-of-the-art performance, outperforming other methods by large margins. To further enhance the performance of {ECCT}, we propose two novel methods. First, leveraging the systematic encoding technique of {ECCs}, we introduce a new masking matrix for {ECCT}, aiming to improve the performance and reduce the computational complexity. Second, we propose a novel transformer architecture of {ECCT} called a double-masked {ECCT}. This architecture employs two different mask matrices in a parallel manner to learn more diverse features of the relationship between codeword bits in the masked self-attention blocks. Extensive simulation results show that the proposed double-masked {ECCT} outperforms the conventional {ECCT}, achieving the state-of-the-art decoding performance with significant margins.},
	number = {{arXiv}:2308.08128},
	publisher = {{arXiv}},
	author = {Park, Seong-Joon and Kwak, Hee-Youl and Kim, Sang-Hyo and Kim, Sunghwan and Kim, Yongjune and No, Jong-Seon},
	urldate = {2024-07-26},
	date = {2023-08-25},
	eprinttype = {arxiv},
	eprint = {2308.08128 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Theory, Computer Science - Machine Learning, have read, presentation, should read},
	file = {arXiv Fulltext PDF:/home/ocell/Zotero/storage/VVSA3IQM/Park et al. - 2023 - How to Mask in Error Correction Code Transformer .pdf:application/pdf;arXiv.org Snapshot:/home/ocell/Zotero/storage/F2BM6GUR/2308.html:text/html},
}

@article{bahlDecodingChannelsInsertions1975a,
  title = {Decoding for Channels with Insertions, Deletions, and Substitutions with Applications to Speech Recognition},
  author = {Bahl, L. and Jelinek, F.},
  year = {1975},
  journal = {IEEE Transactions on Information Theory},
  volume = {21},
  number = {4},
  pages = {404--411},
  doi = {10.1109/TIT.1975.1055419},
  keywords = {citation}
}


@book{jelinekStatisticalMethodsSpeech1998,
  title = {Statistical Methods for Speech Recognition},
  author = {Jelinek, Frederick},
  year = {1998},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  isbn = {0-262-10066-5},
  keywords = {citation}
}



@inproceedings{adamAMethodForStoachsticOptimization,
	title = {Adam: A method for stochastic optimization},
	url = {http://arxiv.org/abs/1412.6980},
	booktitle = {3rd international conference on learning representations, {ICLR} 2015, san diego, {CA}, {USA}, may 7-9, 2015, conference track proceedings},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	editor = {Bengio, Yoshua and {LeCun}, Yann},
	year = {2015},
}


@inproceedings{anselPyTorch2Faster2024a,
  title = {{{PyTorch}} 2: {{Faster}} Machine Learning through Dynamic Python Bytecode Transformation and Graph Compilation},
  booktitle = {Proceedings of the 29th {{ACM}} International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, C. K. and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang, Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
  year = {2024},
  series = {Asplos '24},
  pages = {929--947},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3620665.3640366},
  abstract = {This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27{\texttimes} inference and 1.41{\texttimes} training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.},
  isbn = {9798400703850},
  keywords = {citation}
}


@article{richardsonDesignCapacityapproachingIrregular2001,
	title = {Design of capacity-approaching irregular low-density parity-check codes},
	volume = {47},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/document/910578/?arnumber=910578},
	doi = {10.1109/18.910578},
	abstract = {We design low-density parity-check ({LDPC}) codes that perform at rates extremely close to the Shannon capacity. The codes are built from highly irregular bipartite graphs with carefully chosen degree patterns on both sides. Our theoretical analysis of the codes is based on the work of Richardson and Urbanke (see ibid., vol.47, no.2, p.599-618, 2000). Assuming that the underlying communication channel is symmetric, we prove that the probability densities at the message nodes of the graph possess a certain symmetry. Using this symmetry property we then show that, under the assumption of no cycles, the message densities always converge as the number of iterations tends to infinity. Furthermore, we prove a stability condition which implies an upper bound on the fraction of errors that a belief-propagation decoder can correct when applied to a code induced from a bipartite graph with a given degree distribution. Our codes are found by optimizing the degree structure of the underlying graphs. We develop several strategies to perform this optimization. We also present some simulation results for the codes found which show that the performance of the codes is very close to the asymptotic theoretical bounds.},
	pages = {619--637},
	number = {2},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {Richardson, T.J. and Shokrollahi, M.A. and Urbanke, R.L.},
	urldate = {2025-01-13},
	date = {2001-02},
	note = {Conference Name: {IEEE} Transactions on Information Theory},
	keywords = {citation, Error detection coding},
	file = {Full Text PDF:/home/ocell/Zotero/storage/Y89EPWJZ/Richardson et al. - 2001 - Design of capacity-approaching irregular low-densi.pdf:application/pdf;IEEE Xplore Abstract Record:/home/ocell/Zotero/storage/FYWF5WDI/910578.html:text/html},
}


@article{shannonMathematicalTheoryCommunication1948,
	title = {A mathematical theory of communication},
	volume = {27},
	issn = {0005-8580},
	url = {https://ieeexplore.ieee.org/document/6773024},
	doi = {10.1002/j.1538-7305.1948.tb01338.x},
	abstract = {The recent development of various methods of modulation such as {PCM} and {PPM} which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
	pages = {379--423},
	number = {3},
	journaltitle = {The Bell System Technical Journal},
	author = {Shannon, C. E.},
	urldate = {2025-01-13},
	date = {1948-07},
	note = {Conference Name: The Bell System Technical Journal},
	keywords = {citation},
	file = {Full Text PDF:/home/ocell/Zotero/storage/JK56YQY2/Shannon - 1948 - A mathematical theory of communication.pdf:application/pdf;IEEE Xplore Abstract Record:/home/ocell/Zotero/storage/AFBYJ6KQ/6773024.html:text/html},
}


@article{milenkovicDNAbasedDataStorage2024a,
  title = {{{DNA-based}} Data Storage Systems: A Review of Implementations and Code Constructions},
  author = {Milenkovic, Olgica and Pan, Chao},
  year = {2024},
  journal = {IEEE Transactions on Communications},
  volume = {72},
  number = {7},
  pages = {3803--3828},
  doi = {10.1109/TCOMM.2024.3367748},
  keywords = {Carbon,citation,Coded trace reconstruction,coding for DNA profiles,Computer architecture,DNA,DNA-based data storage,Encoding,Reviews,Sequential analysis,string reconstruction,Sugar}
}



@thesis{daveyErrorcorrectionUsingLowDensity,
	title = {Error-correction using Low-Density Parity-Check Codes},
	url = {https://www.inference.org.uk/mcdavey/papers/davey_phd.html},
	type = {phdthesis},
	author = {Davey, M.C.},
	urldate = {2024-07-28},
	file = {davey_phd.ps:/home/ocell/Zotero/storage/5T8XL5FK/davey_phd.ps:application/postscript;Error-correction using Low-Density Parity-Check Codes:/home/ocell/Zotero/storage/XEIWYN8R/davey_phd.html:text/html},
}


@inproceedings{dosovitskiyImageWorth16x162020,
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	url = {https://openreview.net/forum?id=YicbFdNTTy},
	shorttitle = {An Image is Worth 16x16 Words},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on {CNNs} is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks ({ImageNet}, {CIFAR}-100, {VTAB}, etc.), Vision Transformer ({ViT}) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	eventtitle = {International Conference on Learning Representations},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	urldate = {2025-01-13},
	date = {2020-10-02},
	langid = {english},
	file = {Full Text PDF:/home/ocell/Zotero/storage/EBPV4282/Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf},
}

@article{gallagerLowdensityParitycheckCodes1962a,
  title = {Low-Density Parity-Check Codes},
  author = {Gallager, R.},
  year = {1962},
  month = jan,
  journal = {IRE Transactions on Information Theory},
  volume = {8},
  number = {1},
  pages = {21--28},
  issn = {2168-2712},
  doi = {10.1109/TIT.1962.1057683},
  urldate = {2025-01-16},
  abstract = {A low-density parity-check code is a code specified by a parity-check matrix with the following properties: each column contains a small fixed numberj {\textbackslash}geq 3of l's and each row contains a small fixed numberk {$>$} jof l's. The typical minimum distance of these codes increases linearly with block length for a fixed rate and fixedj. When used with maximum likelihood decoding on a sufficiently quiet binary-input symmetric channel, the typical probability of decoding error decreases exponentially with block length for a fixed rate and fixedj. A simple but nonoptimum decoding scheme operating directly from the channel a posteriori probabilities is described. Both the equipment complexity and the data-handling capacity in bits per second of this decoder increase approximately linearly with block length. Forj {$>$} 3and a sufficiently low rate, the probability of error using this decoder on a binary symmetric channel is shown to decrease at least exponentially with a root of the block length. Some experimental results show that the actual probability of decoding error is much smaller than this theoretical bound.},
  keywords = {Channel capacity,Communication systems,Data communication,Equations,Error correction codes,Error probability,Information theory,Linear approximation,Maximum likelihood decoding,Parity check codes},
  file = {/home/ocell/Zotero/storage/56CDAMTR/1057683.html}
}

@article{sellersBitLossGain1962,
  title = {Bit Loss and Gain Correction Code},
  author = {Sellers, F.},
  year = {1962},
  month = jan,
  journal = {IRE Transactions on Information Theory},
  volume = {8},
  number = {1},
  pages = {35--38},
  issn = {2168-2712},
  doi = {10.1109/TIT.1962.1057684},
  urldate = {2025-01-13},
  abstract = {A block code is presented that will correct an error consisting of the gain or loss of a bit (binary digit) within the block. The code can be generalized to correct the loss or gain of a burst of bits. A further feature is the possibility of correcting additive errors appearing in the vicinity of the bit loss or gain. An additive error is a bit changed from 0 to 1 or from 1 to 0. The code is constructed by inserting a known character into a burst-error-correcting code at periodic intervals. The known character locates the approximate position of the bit loss or gain. At the location a bit is inserted or removed from the block, depending on whether a loss or a gain has occurred. The error-correcting code then corrects the erroneous bits between where the error occurred and where the correction took place.},
  keywords = {Additives,Block codes,citation,Data communication,Data systems,Decoding,Digital communication,Error correction,Error correction codes,Propagation losses,Transmitters},
  file = {/home/ocell/Zotero/storage/4EYJV55L/Sellers - 1962 - Bit loss and gain correction code.pdf;/home/ocell/Zotero/storage/XGWCCKXG/1057684.html}
}

@inproceedings{nachmaniLearningDecodeLinear2016a,
  title = {Learning to Decode Linear Codes Using Deep Learning},
  booktitle = {2016 54th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}} ({{Allerton}})},
  author = {Nachmani, Eliya and Be'ery, Yair and Burshtein, David},
  year = {2016},
  month = sep,
  pages = {341--346},
  doi = {10.1109/ALLERTON.2016.7852251},
  urldate = {2025-01-17},
  abstract = {A novel deep learning method for improving the belief propagation algorithm is proposed. The method generalizes the standard belief propagation algorithm by assigning weights to the edges of the Tanner graph. These edges are then trained using deep learning techniques. A well-known property of the belief propagation algorithm is the independence of the performance on the transmitted codeword. A crucial property of our new method is that our decoder preserved this property. Furthermore, this property allows us to learn only a single codeword instead of exponential number of codewords. Improvements over the belief propagation algorithm are demonstrated for various high density parity check codes.},
  keywords = {Belief propagation,citation,Machine learning,Maximum likelihood decoding,Neural networks,Parity check codes,Training},
  file = {/home/ocell/Zotero/storage/5WE84RPP/Nachmani et al. - 2016 - Learning to decode linear codes using deep learnin.pdf;/home/ocell/Zotero/storage/VAX5XB4J/7852251.html}
}

@article{nachmaniDeepLearningMethods2018,
  title = {Deep {{Learning Methods}} for {{Improved Decoding}} of {{Linear Codes}}},
  author = {Nachmani, Eliya and Marciano, Elad and Lugosch, Loren and Gross, Warren J. and Burshtein, David and Beery, Yair},
  year = {2018},
  month = feb,
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {12},
  number = {1},
  eprint = {1706.07043},
  primaryclass = {cs, math},
  pages = {119--131},
  issn = {1932-4553, 1941-0484},
  doi = {10.1109/JSTSP.2017.2788405},
  urldate = {2024-08-09},
  abstract = {The problem of low complexity, close to optimal, channel decoding of linear codes with short to moderate block length is considered. It is shown that deep learning methods can be used to improve a standard belief propagation decoder, despite the large example space. Similar improvements are obtained for the min-sum algorithm. It is also shown that tying the parameters of the decoders across iterations, so as to form a recurrent neural network architecture, can be implemented with comparable results. The advantage is that significantly less parameters are required. We also introduce a recurrent neural decoder architecture based on the method of successive relaxation. Improvements over standard belief propagation are also observed on sparser Tanner graph representations of the codes. Furthermore, we demonstrate that the neural belief propagation decoder can be used to improve the performance, or alternatively reduce the computational complexity, of a close to optimal decoder of short BCH codes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {citation,Computer Science - Information Theory,Computer Science - Neural and Evolutionary Computing},
  file = {/home/ocell/Zotero/storage/PB7KSITY/Nachmani et al. - 2018 - Deep Learning Methods for Improved Decoding of Lin.pdf}
}

@article{talPolarCodesDeletion2022,
  title = {Polar {{Codes}} for the {{Deletion Channel}}: {{Weak}} and {{Strong Polarization}}},
  shorttitle = {Polar {{Codes}} for the {{Deletion Channel}}},
  author = {Tal, Ido and Pfister, Henry D. and Fazeli, Arman and Vardy, Alexander},
  year = {2022},
  month = apr,
  journal = {IEEE Transactions on Information Theory},
  volume = {68},
  number = {4},
  pages = {2239--2265},
  issn = {1557-9654},
  doi = {10.1109/TIT.2021.3136440},
  urldate = {2025-01-17},
  abstract = {This paper presents the first proof of polarization for the deletion channel with a constant deletion rate and a regular hidden-Markov input distribution. A key part of this work involves representing the deletion channel using a trellis and describing the plus and minus polar-decoding operations on that trellis. In particular, the plus and minus operations can be seen as combining adjacent trellis stages to yield a new trellis with half as many stages. Using this viewpoint, we prove a weak polarization theorem for standard polar codes on the deletion channel. To achieve strong polarization, we modify this scheme by adding guard bands of repeated zeros between various parts of the codeword. This gives a scheme whose rate approaches the mutual information and whose probability of error decays exponentially in the cube-root of the block length. We conclude by showing that this scheme can achieve capacity on the deletion channel by proving that the capacity of the deletion channel can be achieved by a sequence of regular hidden-Markov input distributions.},
  keywords = {channels with memory,citation,Complexity theory,Decoding,deletion channel,Encoding,fast polarization,Markov processes,Mutual information,Polar codes,Standards,Transforms},
  file = {/home/ocell/Zotero/storage/3UH27EF9/Tal et al. - 2022 - Polar Codes for the Deletion Channel Weak and Str.pdf;/home/ocell/Zotero/storage/MM7M5FMV/9654208.html}
}

@article{wangSymbolLevelSynchronizationLDPC2011a,
  title = {Symbol-{{Level Synchronization}} and {{LDPC Code Design}} for {{Insertion}}/{{Deletion Channels}}},
  author = {Wang, Feng and Fertonani, Dario and Duman, Tolga M.},
  year = {2011},
  month = may,
  journal = {IEEE Transactions on Communications},
  volume = {59},
  number = {5},
  pages = {1287--1297},
  issn = {1558-0857},
  doi = {10.1109/TCOMM.2011.030411.100546},
  urldate = {2025-01-17},
  abstract = {We investigate a promising coding scheme over channels impaired by insertion, deletion, and substitution errors, i.e., interleaved concatenation of an outer low-density parity-check (LDPC) code with error-correction capabilities and an inner marker code for synchronization purposes. To limit the decoding latency, we start with a single-pass decoding algorithm, that is, marker code-based synchronization is performed only once per received packet and iterative decoding with information exchange between the inner decoder and outer decoder is not allowed. Through numerical evaluations, we first find the marker code structures which offer the ultimate achievable rate when standard bit-level synchronization is performed. Then, to exploit the correlations in the likelihoods corresponding to different transmitted bits, we introduce a novel symbol-level synchronization algorithm that works on groups of consecutive bits, and show how it improves the achievable rate along with the error rate performance by capturing part of the rate loss due to interleaving. When decoding latency is not an issue and multiple-pass decoding is performed, we utilize extrinsic information transfer (EXIT) charts to analyze the convergence behavior of the receiver, which leads to design of outer LDPC codes with good degree distributions. Finally, design examples are provided along with simulation results which confirm the advantage of the newly designed codes over the ones optimized for the standard additive white Gaussian noise (AWGN) channels, especially for channels with severe synchronization problems.},
  keywords = {citation,Decoding,Detectors,Encoding,Insertion/deletion channel,Iterative decoding,LDPC code design,marker codes,Receivers,synchronization,Synchronization},
  file = {/home/ocell/Zotero/storage/5CXEHGCZ/Wang et al. - 2011 - Symbol-Level Synchronization and LDPC Code Design .pdf;/home/ocell/Zotero/storage/MBRWW63Q/5733455.html}
}

@misc{hoffmannTrainingComputeOptimalLarge2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15556},
  eprint = {2203.15556},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.15556},
  urldate = {2025-01-17},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/ocell/Zotero/storage/ISFRQYXV/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf;/home/ocell/Zotero/storage/2YXMWPDT/2203.html}
}
@misc{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = {2020},
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.08361},
  urldate = {2025-01-17},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {citation,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/ocell/Zotero/storage/57ZSQ48Y/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf;/home/ocell/Zotero/storage/36TEW7CS/2001.html}
}

@misc{channelcodes,
	author = "Helmling, Michael and Scholl, Stefan and Gensheimer, Florian and Dietz, Tobias and Kraft, Kira and Ruzika, Stefan and Wehn, Norbert",
	title = "{D}atabase of {C}hannel {C}odes and {ML} {S}imulation {R}esults",
	howpublished = "\url{www.uni-kl.de/channel-codes}",
	year = "2019"
}

@misc{mackayEncyclopediaSparseGraph,
  title = {Encyclopedia of {{Sparse Graph Codes}}},
  author = {Mackay, D.J.C.},
  urldate = {2025-01-17},
  howpublished = {https://www.inference.org.uk/mackay/codes/data.html},
  file = {/home/ocell/Zotero/storage/WMTI8RQ8/data.html}
}

@inproceedings{shlezingerDatadrivenFactorGraphs2020b,
  title = {Data-Driven Factor Graphs for Deep Symbol Detection},
  booktitle = {2020 {{IEEE}} International Symposium on Information Theory ({{ISIT}})},
  author = {Shlezinger, Nir and Farsad, Nariman and Eldar, Yonina C. and Goldsmith, Andrea J.},
  year = {2020},
  pages = {2682--2687},
  doi = {10.1109/ISIT44484.2020.9174361},
  keywords = {citation}
}

@article{sazliNeuralNetworkImplementation2007,
  title = {Neural Network Implementation of the {{BCJR}} Algorithm},
  author = {Sazl{\i}, Murat H{\"u}sn{\"u} and I{\c s}{\i}k, Can},
  year = {2007},
  journal = {Digital Signal Processing},
  volume = {17},
  number = {1},
  pages = {353--359},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2005.12.002},
  abstract = {In this paper, we first show that the BCJR algorithm (or Bahl algorithm) can be implemented via some matrix manipulations. As a direct result of this, we also show that this algorithm is equivalent to a feedforward neural network structure. We verified through computer simulations that this novel neural network implementation yields identical results with the BCJR algorithm.},
  keywords = {Bahl algorithm,BCJR algorithm,Channel coding,citation,MAP algorithm,Neural networks,Turbo codes,Turbo coding/decoding}
}

@inproceedings{farsadDatadrivenSymbolDetection2021,
  title = {Data-Driven Symbol Detection via Model-Based Machine Learning},
  booktitle = {2021 {{IEEE}} Statistical Signal Processing Workshop ({{SSP}})},
  author = {Farsad, Nariman and Shlezinger, Nir and Goldsmith, Andrea J. and Eldar, Yonina C.},
  year = {2021},
  pages = {571--575},
  doi = {10.1109/SSP49050.2021.9513859},
  keywords = {citation,Computational modeling,Machine learning algorithms,Receivers,Signal processing algorithms,Training data,Uncertainty,Viterbi algorithm}
}

@inproceedings{shlezingerViterbiNetSymbolDetection2019,
  title = {{{ViterbiNet}}: {{Symbol}} Detection Using a Deep Learning Based Viterbi Algorithm},
  booktitle = {2019 {{IEEE}} 20th International Workshop on Signal Processing Advances in Wireless Communications ({{SPAWC}})},
  author = {Shlezinger, Nir and Eldar, Yonina C. and Farsad, Nariman and Goldsmith, Andrea J.},
  year = {2019},
  pages = {1--5},
  doi = {10.1109/SPAWC.2019.8815457}
}
@article{organickRandomAccessLargescale2018,
  title = {Random Access in Large-Scale {{DNA}} Data Storage},
  author = {Organick, Lee and Ang, Siena Dumas and Chen, Yuan-Jyue and Lopez, Randolph and Yekhanin, Sergey and Makarychev, Konstantin and Racz, Miklos Z and Kamath, Govinda and Gopalan, Parikshit and Nguyen, Bichlien and Takahashi, Christopher N and Newman, Sharon and Parker, Hsing-Yeh and Rashtchian, Cyrus and Stewart, Kendall and Gupta, Gagan and Carlson, Robert and Mulligan, John and Carmean, Douglas and Seelig, Georg and Ceze, Luis and Strauss, Karin},
  year = {2018},
  month = mar,
  journal = {Nature Biotechnology},
  volume = {36},
  number = {3},
  pages = {242--248},
  issn = {1546-1696},
  doi = {10.1038/nbt.4079},
  abstract = {200 MB of digital data is stored in DNA, randomly accessed and recovered using an error-free approach.}
}
@misc{barlev2024deepdnastoragescalable,
  title = {Deep {{DNA}} Storage: {{Scalable}} and Robust {{DNA}} Storage via Coding Theory and Deep Learning},
  author = {{Bar-Lev}, Daniella and Orr, Itai and Sabary, Omer and Etzion, Tuvi and Yaakobi, Eitan},
  year = {2024},
  eprint = {2109.00031},
  primaryclass = {cs.IT},
  archiveprefix = {arXiv}
}

@article{heckelCharacterizationDNAData2019,
  title = {A {{Characterization}} of the {{DNA Data Storage Channel}}},
  author = {Heckel, Reinhard and Mikutis, Gediminas and Grass, Robert N.},
  year = {2019},
  month = jul,
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {9663},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-45832-6},
  abstract = {Owing to its longevity and enormous information density, DNA, the molecule encoding biological information, has emerged as a promising archival storage medium. However, due to technological constraints, data can only be written onto many short DNA molecules that are stored in an unordered way, and can only be read by sampling from this DNA pool. Moreover, imperfections in writing (synthesis), reading (sequencing), storage, and handling of the DNA, in particular amplification via PCR, lead to a loss of DNA molecules and induce errors within the molecules. In order to design DNA storage systems, a qualitative and quantitative understanding of the errors and the loss of molecules is crucial. In this paper, we characterize those error probabilities by analyzing data from our own experiments as well as from experiments of two different groups. We find that errors within molecules are mainly due to synthesis and sequencing, while imperfections in handling and storage lead to a significant loss of sequences. The aim of our study is to help guide the design of future DNA data storage systems by providing a quantitative and qualitative understanding of the DNA data storage channel.},
  keywords = {citation}
}

@article{tannerRecursiveApproachLow1981,
  title = {A Recursive Approach to Low Complexity Codes},
  author = {Tanner, R.},
  year = {1981},
  journal = {IEEE Transactions on Information Theory},
  volume = {27},
  number = {5},
  pages = {533--547},
  doi = {10.1109/TIT.1981.1056404},
  keywords = {citation}
}

@article{shlezingerModelbasedDeepLearning2023,
  title = {Model-Based Deep Learning},
  author = {Shlezinger, Nir and Whang, Jay and Eldar, Yonina C. and Dimakis, Alexandros G.},
  year = {2023},
  journal = {Proceedings of the IEEE},
  volume = {111},
  number = {5},
  pages = {465--499},
  doi = {10.1109/JPROC.2023.3247480},
  keywords = {citation,Computational modeling,Data models,Deep learning,Machine learning,Mathematical models,model-based machine learning,signal processing,Signal processing,Signal processing algorithms}
}

@article{antkowiakLowCostDNA2020,
  title = {Low Cost {{DNA}} Data Storage Using Photolithographic Synthesis and Advanced Information Reconstruction and Error Correction},
  author = {Antkowiak, Philipp L. and Lietard, Jory and Darestani, Mohammad Zalbagi and Somoza, Mark M. and Stark, Wendelin J. and Heckel, Reinhard and Grass, Robert N.},
  year = {2020},
  month = oct,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {5345},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-19148-3},
  abstract = {Due to its longevity and enormous information density, DNA is an attractive medium for archival storage. The current hamstring of DNA data storage systems---both in cost and speed---is synthesis. The key idea for breaking this bottleneck pursued in this work is to move beyond the low-error and expensive synthesis employed almost exclusively in today's systems, towards cheaper, potentially faster, but high-error synthesis technologies. Here, we demonstrate a DNA storage system that relies on massively parallel light-directed synthesis, which is considerably cheaper than conventional solid-phase synthesis. However, this technology has a high sequence error rate when optimized for speed. We demonstrate that even in this high-error regime, reliable storage of information is possible, by developing a pipeline of algorithms for encoding and reconstruction of the information. In our experiments, we store a file containing sheet music of Mozart, and show perfect data recovery from low synthesis fidelity DNA.},
  keywords = {citation}
}

@inproceedings{srinivasavaradhanTrellisBMACoded2021,
  title = {Trellis {{BMA}}: {{Coded}} Trace Reconstruction on {{IDS}} Channels for {{DNA}} Storage},
  booktitle = {2021 {{IEEE}} International Symposium on Information Theory ({{ISIT}})},
  author = {Srinivasavaradhan, Sundara Rajan and Gopi, Sivakanth and Pfister, Henry D. and Yekhanin, Sergey},
  year = {2021},
  pages = {2453--2458},
  doi = {10.1109/ISIT45174.2021.9517821},
  keywords = {citation,Clustering algorithms,DNA,Encoding,Error analysis,Reconstruction algorithms,Redundancy,Sequential analysis},
  file = {/home/ocell/Zotero/storage/KDAI35A9/Srinivasavaradhan et al. - 2021 - Trellis BMA Coded trace reconstruction on IDS cha.pdf}
}

@article{butlerBoundsMinimumDistance2013,
  title = {Bounds on the Minimum Distance of Punctured Quasi-Cyclic {{LDPC}} Codes},
  author = {Butler, Brian K. and Siegel, Paul H.},
  year = {2013},
  journal = {IEEE Transactions on Information Theory},
  volume = {59},
  number = {7},
  pages = {4584--4597},
  doi = {10.1109/TIT.2013.2253152},
  keywords = {Binary codes,block codes,Block codes,citation,error correction codes,linear codes,Parity check codes,Polynomials,sparse matrices,Sparse matrices,Standards,Upper bound,Vectors}
}

@article{mitzenmacherSurveyResultsDeletion2009a,
  title = {A Survey of Results for Deletion Channels and Related Synchronization Channels},
  author = {Mitzenmacher, Michael},
  year = {2009},
  journal = {Probability Surveys},
  volume = {6},
  number = {none},
  pages = {1--33},
  doi = {10.1214/08-PS141},
  keywords = {capacity bounds,citation,Deletion channels,random subsequences,synchronization channels}
}


@incollection{sloaneSingledeletioncorrectingCodes2002a,
  title = {On Single-Deletion-Correcting Codes},
  booktitle = {Codes and Designs},
  author = {Sloane, N. J. A.},
  editor = {Arasu, K.T. and Seress, {\'A}koss},
  year = {2002},
  pages = {273--292},
  publisher = {De Gruyter},
  address = {Berlin, New York},
  urldate = {2025-02-06},
  isbn = {978-3-11-019811-9}
}

@inproceedings{buttigiegImprovedBitError2015,
  title = {Improved Bit Error Rate Performance of Convolutional Codes with Synchronization Errors},
  booktitle = {2015 {{IEEE}} International Conference on Communications ({{ICC}})},
  author = {Buttigieg, Victor and Farrugia, Noel},
  year = {2015},
  pages = {4077--4082},
  doi = {10.1109/ICC.2015.7248962},
  keywords = {citation,Convolutional Codes,Decoding,Insertion-Deletion Correction,Quality of service,Synchronization}
}
